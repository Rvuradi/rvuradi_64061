---
title: "AML_ASSIGNMENT 1"
author: "ROHIT VURADI"
date: "2024-02-22"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE, message=FALSE}
library(keras)
library(tensorflow)
library(ggplot2)
```

```{r}
# Load the IMDB dataset
imdb_data <- dataset_imdb(num_words = 10000)
train_data <- imdb_data$train$x
train_labels <- imdb_data$train$y
test_data <- imdb_data$test$x
test_labels <- imdb_data$test$y
```

```{r}
# display the data
str(train_data)
str(train_labels)
str(test_data)
str(test_labels)
```

```{r}
# Prepare the data
vectorize_sequences <- function(sequences, dimension = 10000) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in seq_along(sequences)) {
    results[i, sequences[[i]]] <- 1
  }
  results
}

x_train <- vectorize_sequences(train_data)
x_test <- vectorize_sequences(test_data)
y_train <- as.matrix(train_labels)
y_test <- as.matrix(test_labels)
```

```{r}
# 1. Number of Hidden Layers
# One hidden layer
model_one_hidden <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# Three hidden layers
model_three_hidden <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 16, activation = 'relu') %>%
  layer_dense(units = 1, activation = 'sigmoid')

```

```{r}
# 2. Number of Hidden Units
# 32 hidden units
model_32_units <- keras_model_sequential() %>%
  layer_dense(units = 32, activation = 'relu', input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = 'sigmoid')

# 64 hidden units
model_64_units <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = 'sigmoid')
```

```{r}
# 3. Loss Function
# Mean Squared Error (MSE) loss function
model_mse_loss <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = 'sigmoid')

model_mse_loss %>% compile(optimizer = 'rmsprop', loss = 'mse', metrics = c('accuracy'))
```

```{r}
# 4. Activation Function
# tanh activation function
model_tanh_activation <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'tanh', input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = 'sigmoid')
```

```{r}
# 5. Regularization and Dropout
# L2 Regularization
model_l2_regularization <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(10000), kernel_regularizer = regularizer_l2(0.001)) %>%
  layer_dense(units = 1, activation = 'sigmoid')
```

```{r}
# Dropout
model_dropout <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = 'relu', input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = 'sigmoid')
```

```{r}
# Function to create, compile, and fit the model
compile_and_fit_model <- function(model, x_train, y_train, x_val, y_val, epochs = 20, batch_size = 512) {
  model %>% compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = c('accuracy'))
  history <- model %>% fit(x_train, y_train, epochs = epochs, batch_size = batch_size, validation_data = list(x_val, y_val), verbose = 0)
  history
}
```

```{r}
# Function to plot the training and validation accuracy
plot_history <- function(history, label) {
  acc <- history$metrics$accuracy
  val_acc <- history$metrics$val_accuracy
  epochs <- seq_along(acc)
  plot(epochs, acc, type = 'l', col = 'blue', xlab = 'Epochs', ylab = 'Accuracy', main = paste('Training and Validation Accuracy (', label, ')'))
  lines(epochs, val_acc, type = 'l', col = 'red')
  legend('topright', legend = c(paste('Training Accuracy (', label, ')'), paste('Validation Accuracy (', label, ')')), col = c('blue', 'red'), lty = 1:1)
}
```

```{r}
# Create a list of models
models_list <- list(model_one_hidden, model_three_hidden, model_32_units, model_64_units, model_mse_loss, model_tanh_activation, model_l2_regularization, model_dropout)

# Loop through each model, print summary, compile, fit, and plot
for (i in seq_along(models_list)) {
  label <- paste('Model', i)
  
  # Print model summary
  cat('\n\n', label, 'Summary:')
  summary(models_list[[i]])
  
  # Compile and fit the model
  history <- compile_and_fit_model(models_list[[i]], x_train, y_train, x_test, y_test)
  
  # Plot the training and validation accuracy
  plot_history(history, label)
  
  # Clear session to release memory
  gc()
}
```



1. **Data Loading and Preparation:**
   - The code starts by loading the IMDB dataset using Keras, consisting of movie reviews and their associated sentiment labels (positive or negative).
   - The reviews are vectorized to binary sequences, where each word is represented by a binary indicator.

2. **Model Architectures:**
   - Four different model architectures are defined to explore various configurations:
      - `model_one_hidden`: One hidden layer with 16 units.
      - `model_three_hidden`: Three hidden layers with 16 units each.
      - `model_32_units`: One hidden layer with 32 units.
      - `model_64_units`: One hidden layer with 64 units.
      - `model_mse_loss`: One hidden layer with Mean Squared Error (MSE) loss.
      - `model_tanh_activation`: One hidden layer with tanh activation.
      - `model_l2_regularization`: One hidden layer with L2 regularization.
      - `model_dropout`: One hidden layer with dropout.

3. **Model Compilation, Training, and Plotting:**
   - A function `compile_and_fit_model` is defined to compile and fit a model with binary cross-entropy loss and RMSprop optimizer.
   - Another function `plot_history` is defined to plot the training and validation accuracy over epochs.
   - The code then iterates through each model, prints its summary, compiles, fits on the training data, and plots the training and validation accuracy.

4. **Results Visualization:**
   - Finally, a plot is generated to compare the training and validation accuracy of different model configurations over epochs.

5. **Observations:**
   - The code aims to explore how different architectural choices impact the model's training and validation performance on the IMDB sentiment analysis task.
